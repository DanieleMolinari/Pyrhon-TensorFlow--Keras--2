{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSC_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUrixIjpufxK"
      },
      "source": [
        "#Uploading the data.\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/'\n",
        "datafile = url + 'UCI%20HAR%20Dataset.zip'\n",
        "!curl $datafile --output UCI_HAR_Dataset.zip\n",
        "!unzip -qq UCI_HAR_Dataset.zip\n",
        "!mv -f UCIn HARn Dataset UCI_HAR_DATASETY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn9f6t8HwsX_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVrFHj1SvMp_"
      },
      "source": [
        "# load the features and labels (subtract 1 as the labels aren't indexed from 0)\n",
        "ytest = np.loadtxt('UCI HAR Dataset/test/y_test.txt')-1\n",
        "ytrain = np.loadtxt('UCI HAR Dataset/train/y_train.txt')-1\n",
        "# load the x,y,z body accelerations test data\n",
        "xx=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt')\n",
        "yy=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt')\n",
        "zz=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt')\n",
        "# concatenate the arrays along the last dimension\n",
        "xtest = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None]),axis=2)\n",
        "# (using None here adds an extra dimension of size 1 to the end of the array)\n",
        "# follow the same approach for the train data\n",
        "xx=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt')\n",
        "yy=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt')\n",
        "zz=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt')\n",
        "xtrain = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None]),axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJzNkGVyQ8V"
      },
      "source": [
        "print(ytest.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtest.shape)\n",
        "print(xtrain.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JC6q5XSyvTn"
      },
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(xtrain, ytrain, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz0GLkeSkRKN"
      },
      "source": [
        "print(xvalid.shape)\n",
        "print(yvalid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk6om8jokhF7"
      },
      "source": [
        "ytrain_in = tf.keras.utils.to_categorical(ytrain)\n",
        "yvalid_in = tf.keras.utils.to_categorical(yvalid)\n",
        "\n",
        "xtrain_in = (np.reshape(xtrain, (xtrain.shape[0], 384))).astype(np.float32)\n",
        "xvalid_in = (np.reshape(xvalid, (xvalid.shape[0], 384))).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW-QXlA_yPSq"
      },
      "source": [
        "print(xtrain_in.shape)\n",
        "print(ytrain_in.shape)\n",
        "print(xvalid_in.shape)\n",
        "print(yvalid_in.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwxL7pI-ykF4"
      },
      "source": [
        "print(ytrain_in)\n",
        "print(yvalid_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2vs-EIcyzpo"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMHHc8pD5AcY"
      },
      "source": [
        "print(W)\n",
        "print(W.shape)\n",
        "print(b.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHoFuVbizY9P"
      },
      "source": [
        "def y_pred (x):\n",
        "  return tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "def loss(x,y):\n",
        "    y_ = y_pred(x)\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_), axis=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0U1UIId0NnD"
      },
      "source": [
        "logdir = 'tflogs'\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l88YZhxk0ZLy"
      },
      "source": [
        "train_steps = 10000\n",
        "lr = 1e-3\n",
        "optimizer = tf.optimizers.SGD(lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byo7xrT_bf5O"
      },
      "source": [
        "def accuracy (x, y):\n",
        "  y_ = y_pred(x)\n",
        "  correct = tf.math.equal(tf.math.argmax(y_, axis = -1), tf.math.argmax(y, axis = -1))\n",
        "  correct = tf.cast(correct, dtype = tf.float32)\n",
        "  return tf.math.reduce_mean(correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pwh_A7x0OKz"
      },
      "source": [
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)        \n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)  \n",
        "    if i%100 == 0:\n",
        "      print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml8Aw6Z17h_3"
      },
      "source": [
        "#Activation of Tensorboard and plot of the loss function.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53KIsn0q9Vza"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gTqZVim_m8b"
      },
      "source": [
        "#Trying Adam optimizer\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE-2x46A_ubJ"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRwO7e1OjayE"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGZodD1itE8"
      },
      "source": [
        "#Trying 5000 train steps\n",
        "train_steps = 5000\n",
        "lr = 1e-3\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJFTZAVWitCW"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMMKfrUgrK_1"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gTNxUHpisun"
      },
      "source": [
        "#Trying 15000 train steps\n",
        "train_steps = 15000\n",
        "lr = 1e-3\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83kMwCeLisl8"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN6GWdj6mmkj"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hCt-RRmmiu"
      },
      "source": [
        "#Trying Learning rate 0.1\n",
        "train_steps = 5000\n",
        "lr = 1e-1\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9C8FGqMmmfD"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYC2Iox0mmbr"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0w8XeHvliux"
      },
      "source": [
        "#Trying Learning rate 0.01\n",
        "train_steps = 5000\n",
        "lr = 1e-2\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3oW3wIlil4"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsEgRxdslidR"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFmiN58aliTA"
      },
      "source": [
        "#Trying Learning rate 0.0001\n",
        "train_steps = 5000\n",
        "lr = 1e-4\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)     \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz6PCcs_ndjo"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AyfdL2Dk9UG"
      },
      "source": [
        "W = tf.Variable(tf.zeros([384, 6], name = 'W'))\n",
        "b = tf.Variable(tf.zeros([6]), name = 'b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d45AEq7oXKC"
      },
      "source": [
        "#chosen model\n",
        "train_steps = 5000\n",
        "lr = 1e-3\n",
        "optimizer = tf.optimizers.Adam(lr)\n",
        "\n",
        "with writer.as_default():\n",
        "  for i in range(train_steps):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = loss(xtrain_in, ytrain_in)\n",
        "    gradients = tape.gradient(current_loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W ,b]))\n",
        "    tf.summary.scalar('current_loss', current_loss, step = i)\n",
        "    acc = accuracy(xtrain_in, ytrain_in)\n",
        "    tf.summary.scalar('acc', acc, step = i)      \n",
        "    if i%100 == 0:\n",
        "       print('Training Step:' + str(i) + '  Loss = ' + str(current_loss) + '  Accuracy = ' + str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgzxSSn0AoCB"
      },
      "source": [
        "print(accuracy(xtrain_in, ytrain_in))\n",
        "print(accuracy(xvalid_in, yvalid_in))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOb9dsezEvh"
      },
      "source": [
        "#Initial model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 2), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 10, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B2CRAqAyshc"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO_J6xrMig89"
      },
      "source": [
        "#Tuning epochs = 100, patience = 4\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 4), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fap09PO9ihNn"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_CP_zKrihRe"
      },
      "source": [
        "#Tuning epochs = 100, patience = 6\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 6), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2roPEm3ihUx"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5oaaGJBihXw"
      },
      "source": [
        "#Tuning epochs = 100, patience = 8\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 8), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ebr7fdQihae"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UypkDq5yihdN"
      },
      "source": [
        "#Tuning epochs = 100, patience = 10\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp8GVjUOihfs"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htasRfiLihh_"
      },
      "source": [
        "#Tuning epochs = 100, patience = 15\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6qqyXoHpDAn"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN8inbKCuAOC"
      },
      "source": [
        "#Tuning epochs = 100, patience = 20\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 20), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlSjgL9nuAEp"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiWkfzFkxLNM"
      },
      "source": [
        "#Epochs = 100 with patience 15, tuning batch size = 16\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 16, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPkDJxD1zWV2"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zinJy9L12KAV"
      },
      "source": [
        "#Epochs = 100 with patience 15, tuning batch size = 64\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 64, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Zg61bL2KG_"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8RJgr6u3gB7"
      },
      "source": [
        "#Epochs = 100 with patience = 15, tuning batch size = 128\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 128, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uWB05qs3gFJ"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQhrXy-44mU"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, tuning learning rate = 0.1\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.1), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGAZVqYiILjE"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ0DXUwy44pY"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, tuning learning rate = 0.01\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLiMl2C1IOts"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgLcz0KF6jdr"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, tuning learning rate = 0.001\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnPQKvXyIQ1E"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fha3SCwB6jgR"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, tuning learning rate = 0.0001\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 4, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod12 = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod12 == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "765KUeUiIS2P"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wljtb8RCMy7z"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.MaxPool1D())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAi3LXp1Mywi"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBRdvI_1zMYG"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.MaxPool1D())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sb_UksMMyZO"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW8eMAEw6jmk"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 3)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model.add(tf.keras.layers.MaxPool1D())\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(xtrain, ytrain_in, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid, yvalid_in))\n",
        "\n",
        "predictions = model.predict(xvalid)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod == yvalid)/len(yvalid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9eM_jxEADu0"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xTIySNTiFAa"
      },
      "source": [
        "ytest = np.loadtxt('UCI HAR Dataset/test/y_test.txt')-1\n",
        "ytrain = np.loadtxt('UCI HAR Dataset/train/y_train.txt')-1\n",
        "\n",
        "xx=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt')\n",
        "yy=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt')\n",
        "zz=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt')\n",
        "xxt=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt')\n",
        "yyt=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt')\n",
        "zzt=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt')\n",
        "xxg=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt')\n",
        "yyg=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt')\n",
        "zzg=np.loadtxt('/content/UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt')\n",
        "\n",
        "xtest2 = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None],\n",
        "xxt[:,:,None],yyt[:,:,None],zzt[:,:,None],\n",
        "xxg[:,:,None],yyg[:,:,None],zzg[:,:,None]),axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvYRvv_AbsZX"
      },
      "source": [
        "xx=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt')\n",
        "yy=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt')\n",
        "zz=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt')\n",
        "xxt=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt')\n",
        "yyt=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt')\n",
        "zzt=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt')\n",
        "xxg=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt')\n",
        "yyg=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt')\n",
        "zzg=np.loadtxt('/content/UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt')\n",
        "\n",
        "xtrain2 = np.concatenate((xx[:,:,None],yy[:,:,None],zz[:,:,None],\n",
        "xxt[:,:,None],yyt[:,:,None],zzt[:,:,None],\n",
        "xxg[:,:,None],yyg[:,:,None],zzg[:,:,None]),axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7npWL_1OhI-j"
      },
      "source": [
        "xtrain2, xvalid2, ytrain2, yvalid2 = train_test_split(xtrain2, ytrain, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGxJUWany84U"
      },
      "source": [
        "ytrain_in2 = tf.keras.utils.to_categorical(ytrain2)\n",
        "yvalid_in2 = tf.keras.utils.to_categorical(yvalid2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOgBLef4c85r"
      },
      "source": [
        "print(ytest.shape)\n",
        "print(xtest2.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtrain2.shape)\n",
        "print(yvalid2.shape)\n",
        "print(xvalid2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN10_GPKvVvc"
      },
      "source": [
        "#Epochs = 100 with patience = 15, batch size = 32, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model2.add(tf.keras.layers.MaxPool1D())\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 8), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model2.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model2.fit(xtrain2, ytrain_in2, epochs = 100, batch_size = 32, callbacks = callbacks, verbose = 0, validation_data = (xvalid2, yvalid_in2))\n",
        "\n",
        "predictions = model2.predict(xvalid2)\n",
        "y_pred_mod2 = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod2 == yvalid2)/len(yvalid2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2SR-B1d3zT6"
      },
      "source": [
        "#Epochs = 100 with patience = 8, batch size = 16, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model2.add(tf.keras.layers.MaxPool1D())\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 8), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model2.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model2.fit(xtrain2, ytrain_in2, epochs = 100, batch_size = 16, callbacks = callbacks, verbose = 0, validation_data = (xvalid2, yvalid_in2))\n",
        "\n",
        "predictions = model2.predict(xvalid2)\n",
        "y_pred_mod2 = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod2 == yvalid2)/len(yvalid2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9512a4b3Gxq"
      },
      "source": [
        "#Epochs = 100 with patience = 8, batch size = 64, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model2.add(tf.keras.layers.MaxPool1D())\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 8), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model2.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model2.fit(xtrain2, ytrain_in2, epochs = 100, batch_size = 64, callbacks = callbacks, verbose = 0, validation_data = (xvalid2, yvalid_in2))\n",
        "\n",
        "predictions = model2.predict(xvalid2)\n",
        "y_pred_mod2 = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod2 == yvalid2)/len(yvalid2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SitFA1ApeLtP"
      },
      "source": [
        "#Epochs = 100 with patience = 8, batch size = 128, learning rate = 0.0001. Two more layers. Double filters and Kernel size.\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 128, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 256, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 8, activation = tf.nn.relu, padding = 'same', input_shape = (128, 9)))\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "model2.add(tf.keras.layers.Conv1D(filters = 64, kernel_size = 8, activation = tf.nn.relu))\n",
        "model2.add(tf.keras.layers.MaxPool1D())\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "model2.add(tf.keras.layers.Dense(units = 6, activation = tf.nn.softmax))\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 8), tf.keras.callbacks.TensorBoard(log_dir=logdir)]\n",
        "\n",
        "model2.compile(optimizer = tf.keras.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "model2.fit(xtrain2, ytrain_in2, epochs = 100, batch_size = 128, callbacks = callbacks, verbose = 0, validation_data = (xvalid2, yvalid_in2))\n",
        "\n",
        "predictions = model2.predict(xvalid2)\n",
        "y_pred_mod2 = np.argmax(predictions, axis = -1)\n",
        "print('Validation Accuracy: ', np.sum(y_pred_mod2 == yvalid2)/len(yvalid2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6GAN0kNkHZY"
      },
      "source": [
        "%tensorboard --logdir tflogs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvgct2dl5JGS"
      },
      "source": [
        "ytest_in = tf.keras.utils.to_categorical(ytest)\n",
        "xtest_in = (np.reshape(xtest, (xtest.shape[0], 384))).astype(np.float32)\n",
        "\n",
        "print('Test Accuracy:', accuracy(xtest_in, ytest_in))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQz6JlAK5JIu"
      },
      "source": [
        "predictions = model.predict(xtest)\n",
        "y_pred_mod = np.argmax(predictions, axis = -1)\n",
        "print('Test Accuracy: ', np.sum(y_pred_mod == ytest)/len(ytest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zseohpiZ5JLi"
      },
      "source": [
        "predictions = model2.predict(xtest2)\n",
        "y_pred_mod2 = np.argmax(predictions, axis = -1)\n",
        "print('Test Accuracy: ', np.sum(y_pred_mod2 == ytest)/len(ytest))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}